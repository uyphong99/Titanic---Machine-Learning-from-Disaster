{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-15T08:33:09.553278Z","iopub.execute_input":"2022-08-15T08:33:09.553710Z","iopub.status.idle":"2022-08-15T08:33:09.738258Z","shell.execute_reply.started":"2022-08-15T08:33:09.553674Z","shell.execute_reply":"2022-08-15T08:33:09.736893Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# API for outlier problem:\n# I refer to the machinelearningcoban blog of author Nguyen Huu Tiep.\n# This is the solution for skewed problem. We will use this to cleaning, processing the numerical variables.\nfrom typing import Tuple\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n\ndef find_boxplot_boundaries(\n    col: pd.Series, whisker_coeff: float = 1.5\n) -> Tuple[float, float]:\n    \"\"\"Findx minimum and maximum in boxplot.\n\n    Args:\n        col: a pandas serires of input.\n        whisker_coeff: whisker coefficient in box plot\n    \"\"\"\n    Q1 = col.quantile(0.25)\n    Q3 = col.quantile(0.75)\n    IQR = Q3 - Q1\n    lower = Q1 - whisker_coeff * IQR\n    upper = Q3 + whisker_coeff * IQR\n    return lower, upper\n\n\nclass BoxplotOutlierClipper(BaseEstimator, TransformerMixin):\n    def __init__(self, whisker_coeff: float = 1.5):\n        self.whisker = whisker_coeff\n        self.lower = None\n        self.upper = None\n\n    def fit(self, X: pd.Series):\n        self.lower, self.upper = find_boxplot_boundaries(X, self.whisker)\n        return self\n\n    def transform(self, X):\n        return X.clip(self.lower, self.upper)","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:33:09.740555Z","iopub.execute_input":"2022-08-15T08:33:09.741138Z","iopub.status.idle":"2022-08-15T08:33:09.750727Z","shell.execute_reply.started":"2022-08-15T08:33:09.741100Z","shell.execute_reply":"2022-08-15T08:33:09.749550Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Training data\ntrain_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:33:09.752179Z","iopub.execute_input":"2022-08-15T08:33:09.752788Z","iopub.status.idle":"2022-08-15T08:33:09.805014Z","shell.execute_reply.started":"2022-08-15T08:33:09.752755Z","shell.execute_reply":"2022-08-15T08:33:09.803857Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Test data set\ntest_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ntest_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:33:09.807646Z","iopub.execute_input":"2022-08-15T08:33:09.808695Z","iopub.status.idle":"2022-08-15T08:33:09.834075Z","shell.execute_reply.started":"2022-08-15T08:33:09.808656Z","shell.execute_reply":"2022-08-15T08:33:09.832766Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# 1. Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"In this step, I will filter out those features that have an impact on passenger survival.","metadata":{}},{"cell_type":"markdown","source":"**For whom who doesn't grab the context, I will explain clearly what are the meaning of some feature(column):**\n* Pclass: Similiar to the class of airline, Titanic have **three class {1, 2, 3}**. This feature potentially affecting passenger survival. Maybe the 1st class would be will be given priority. We will discuss about this further in the notebook, I will not give much assumption soon which could cause bias.\n* sibsp: Number of siblings / spouses aboard the Titanic. This variable also has the potential to influence survival outcomes.\n* Parch: Number of parents and children on board. Same as sibsp.\n* Sex: male or female\n* Ticket: Ticker number\n* Fare: The price of ticket\n* Cabin: Cabin number\n* embarked: Port of Embarkation, C = Cherbourg, Q = Queenstown, S = Southampton\n\nBase on what I explain above, I classify them into 2 type: \n\n**Category features:** Pclass {1,2,3} (We could also treat Pclass as a numerical feature), Sex {'male', 'female'}, embarked {'C', 'Q', 'S'}\n\n**Numerical feature:** sibsp, parch, fare\n\n\nDepending to the choice of the model, we could transform category feature to numerical. I won't transform them until finish the analyse step.\n\nI didn't list ticket and cabin because I believe they won't give much information, **Cabin** may helpful but they have too much null, **Ticket** number could imply the class of people but we already have Pclass and Fare take care that information. Also, data type of ticket is a little bit vague. I will drop Ticket and Cabin column.\n\nI also assume the name column will give us some interesting information, for example, the name have honorific \"Dr\" could have more chance of survival. I will extract the tittle of name to create new feature","metadata":{}},{"cell_type":"code","source":"train_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:33:09.836851Z","iopub.execute_input":"2022-08-15T08:33:09.837404Z","iopub.status.idle":"2022-08-15T08:33:09.856442Z","shell.execute_reply.started":"2022-08-15T08:33:09.837363Z","shell.execute_reply":"2022-08-15T08:33:09.855292Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"**Let us discuss some basic statistics metrics about the training set:**\n* The first thing I notice: Number of count(Age) = 714, show that this is the only column contain NULL value.\n* The mean of survivor = 0.38, that mean the ratio between survivors and unlucky people are relative balance.\n* The SibSp and Parch: Call to mind, they are the number of relatives of a specific person. We see that number of people who have 1 SibSp accounting for 75%, mean that the data is quite skewed. Similar to the Parch column. Another sign is that the different of their mean and median.\n* Fare is skewed, similar to SibSp and Parch.","metadata":{}},{"cell_type":"code","source":"# Drop Ticket and Cabin\ntrain_df = train_data.drop(['Ticket', 'Cabin'], axis=1)\ntest_df = test_data.drop(['Ticket', 'Cabin'], axis=1)\ncombine = [train_df, test_df] # We group them in a list for easy to modify later\n","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:33:09.858298Z","iopub.execute_input":"2022-08-15T08:33:09.858641Z","iopub.status.idle":"2022-08-15T08:33:09.872288Z","shell.execute_reply.started":"2022-08-15T08:33:09.858611Z","shell.execute_reply":"2022-08-15T08:33:09.871233Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"for dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\ntrain_df['Title']","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:33:09.874389Z","iopub.execute_input":"2022-08-15T08:33:09.875187Z","iopub.status.idle":"2022-08-15T08:33:09.896218Z","shell.execute_reply.started":"2022-08-15T08:33:09.875154Z","shell.execute_reply":"2022-08-15T08:33:09.894850Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Some titles are also associated with gender, let's see.\npd.crosstab(train_df['Title'], train_df['Sex'])","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:33:09.897677Z","iopub.execute_input":"2022-08-15T08:33:09.898206Z","iopub.status.idle":"2022-08-15T08:33:09.941568Z","shell.execute_reply.started":"2022-08-15T08:33:09.898175Z","shell.execute_reply":"2022-08-15T08:33:09.940259Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"for dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace(['Mlle','Ms'], 'Miss')\n    \n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean().sort_values(by = ['Survived'])","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:33:09.946051Z","iopub.execute_input":"2022-08-15T08:33:09.946402Z","iopub.status.idle":"2022-08-15T08:33:09.972821Z","shell.execute_reply.started":"2022-08-15T08:33:09.946372Z","shell.execute_reply":"2022-08-15T08:33:09.971694Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Mrs and Miss are the title have the highest survival.","metadata":{}},{"cell_type":"code","source":"# Mapping title to the number in the asc ord, the higher number the higher rate of survival, \n# Mr have the lowest survival rate, it will be mapped to 1, Mrs have the highest rate, we map to 5\ntitle_mapping = {\"Mr\": 1, \"Miss\": 4, \"Mrs\": 5, \"Master\": 3, \"Rare\": 2}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:33:09.997443Z","iopub.execute_input":"2022-08-15T08:33:09.997946Z","iopub.status.idle":"2022-08-15T08:33:10.020022Z","shell.execute_reply.started":"2022-08-15T08:33:09.997904Z","shell.execute_reply":"2022-08-15T08:33:10.019070Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Now we can safely drop name and passengerID, they won't contribute much to training the model\ntrain_df = train_df.drop(['Name', 'PassengerId'], axis=1)\ntest_df = test_df.drop(['Name'], axis=1)\ncombine = [train_df, test_df]\ntrain_df.shape, test_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:33:10.038484Z","iopub.execute_input":"2022-08-15T08:33:10.038940Z","iopub.status.idle":"2022-08-15T08:33:10.051116Z","shell.execute_reply.started":"2022-08-15T08:33:10.038902Z","shell.execute_reply":"2022-08-15T08:33:10.050162Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Combine SibSp and Parch:\n# We could combine them together, they are have the same meaning, \n# their distribution are similar, so it would be great if we create a new feature familySize = SibSp + Parch\nfor dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\ntrain_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:33:10.077566Z","iopub.execute_input":"2022-08-15T08:33:10.078039Z","iopub.status.idle":"2022-08-15T08:33:10.098313Z","shell.execute_reply.started":"2022-08-15T08:33:10.078000Z","shell.execute_reply":"2022-08-15T08:33:10.097054Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"map_family = { 4: 8, 3:7, 2:6, 7:5, 1: 4, 5: 3, 6:2, 8:1, 11: 0}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(map_family)\n    # After create new feature, we can drop the SibSP and Parch\n    dataset.drop(['SibSp', 'Parch'], axis = 1,inplace= True)\n\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:33:10.116369Z","iopub.execute_input":"2022-08-15T08:33:10.117189Z","iopub.status.idle":"2022-08-15T08:33:10.138707Z","shell.execute_reply.started":"2022-08-15T08:33:10.117139Z","shell.execute_reply":"2022-08-15T08:33:10.137749Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"Now, when referring to family size, we talk about people came from a group which have a specific rate of survival .","metadata":{}},{"cell_type":"code","source":"# Processing Embarked\n# We will replace NULL to the most frequent value because this is a category feature, we cannot derive mean or median\nfreq_port = train_df.Embarked.dropna().mode()[0] \nfor dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\ntrain_df[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:33:10.150484Z","iopub.execute_input":"2022-08-15T08:33:10.150971Z","iopub.status.idle":"2022-08-15T08:33:10.171789Z","shell.execute_reply.started":"2022-08-15T08:33:10.150933Z","shell.execute_reply":"2022-08-15T08:33:10.170548Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Same as what I've done before, I will encode Embark to int value, highest survived have highest score\nembarked_mapping = {'C': 3, 'Q': 2, 'S': 1}\nfor dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].map(embarked_mapping)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:33:10.183730Z","iopub.execute_input":"2022-08-15T08:33:10.184132Z","iopub.status.idle":"2022-08-15T08:33:10.202222Z","shell.execute_reply.started":"2022-08-15T08:33:10.184099Z","shell.execute_reply":"2022-08-15T08:33:10.201079Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Transform Sex to binary value\nsex_mapping = {'male': 0, 'female': 1}\nfor dataset in combine:\n    dataset['Sex'] = dataset['Sex'].map(sex_mapping)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:33:10.243710Z","iopub.execute_input":"2022-08-15T08:33:10.244114Z","iopub.status.idle":"2022-08-15T08:33:10.261745Z","shell.execute_reply.started":"2022-08-15T08:33:10.244079Z","shell.execute_reply":"2022-08-15T08:33:10.260538Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"**Now it's time to process the numerical data, we'll check if they have NULL, **","metadata":{}},{"cell_type":"code","source":"# Process the Fare variable:\n\n# Set the size of fig\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\ntrain_df['Fare'].hist(bins=50, ax=axes[0])\ntrain_df['Fare'].to_frame().boxplot(ax=axes[1], vert=False)","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:33:10.314452Z","iopub.execute_input":"2022-08-15T08:33:10.314850Z","iopub.status.idle":"2022-08-15T08:33:10.724122Z","shell.execute_reply.started":"2022-08-15T08:33:10.314817Z","shell.execute_reply":"2022-08-15T08:33:10.722801Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"In the Fare column, we have the right skewed distribution as well as some outliers. Now is the time using the API that I've introduced.","metadata":{}},{"cell_type":"code","source":"test_df['Fare']","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:33:10.726139Z","iopub.execute_input":"2022-08-15T08:33:10.727220Z","iopub.status.idle":"2022-08-15T08:33:10.736625Z","shell.execute_reply.started":"2022-08-15T08:33:10.727171Z","shell.execute_reply":"2022-08-15T08:33:10.735284Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"for dataset in combine:\n    dataset['Fare'] = BoxplotOutlierClipper().fit_transform(dataset['Fare'])\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\ntrain_df['Fare'].hist(bins=50, ax=axes[0])\ntrain_df['Fare'].to_frame().boxplot(ax=axes[1], vert=False)","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:33:10.738377Z","iopub.execute_input":"2022-08-15T08:33:10.738936Z","iopub.status.idle":"2022-08-15T08:33:11.124684Z","shell.execute_reply.started":"2022-08-15T08:33:10.738892Z","shell.execute_reply":"2022-08-15T08:33:11.123536Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"The distribution no longer being skewed, the box plot look nicer. However, it is also not very Gaussian. I'll leave it as it is, after editing the model. The Fearture technique is a lengthy process.\n\n\"Coming up with features is difficult, time-consuming, requires expert knowledge. “Applied machine learning” is basically feature engineering.\" - Andrew Ng","metadata":{}},{"cell_type":"code","source":"test_df['Fare']","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:33:11.127721Z","iopub.execute_input":"2022-08-15T08:33:11.128550Z","iopub.status.idle":"2022-08-15T08:33:11.137280Z","shell.execute_reply.started":"2022-08-15T08:33:11.128489Z","shell.execute_reply":"2022-08-15T08:33:11.136403Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"import scipy\nfrom scipy import stats\nfrom sklearn.impute import SimpleImputer\n\n# Replace NaN Fare with the most frequent value\nimputer = SimpleImputer(strategy='most_frequent')\nimputer.fit(train_df[[\"Fare\"]])\ntrain_df[[\"Fare\"]] = imputer.transform(train_df[[\"Fare\"]])\ntest_df[[\"Fare\"]] = imputer.transform(test_df[[\"Fare\"]])\n\nfor dataset in combine:\n    # transform data\n    dataset['Fare'] = stats.zscore(dataset['Fare']) + 1\n    \ntrain_df['Fare'].head()","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:33:11.138450Z","iopub.execute_input":"2022-08-15T08:33:11.139037Z","iopub.status.idle":"2022-08-15T08:33:11.223540Z","shell.execute_reply.started":"2022-08-15T08:33:11.139004Z","shell.execute_reply":"2022-08-15T08:33:11.222357Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"test_df['Fare']","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:33:11.224964Z","iopub.execute_input":"2022-08-15T08:33:11.225399Z","iopub.status.idle":"2022-08-15T08:33:11.234285Z","shell.execute_reply.started":"2022-08-15T08:33:11.225366Z","shell.execute_reply":"2022-08-15T08:33:11.232922Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"train_df['Fare'].hist(bins=50)","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:33:11.236230Z","iopub.execute_input":"2022-08-15T08:33:11.236627Z","iopub.status.idle":"2022-08-15T08:33:11.507814Z","shell.execute_reply.started":"2022-08-15T08:33:11.236593Z","shell.execute_reply":"2022-08-15T08:33:11.506579Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# simple heatmap to see where we are missing data!\nsns.heatmap(train_data.isnull(),yticklabels=False,cbar=False,cmap='viridis')\nsns.heatmap(test_data.isnull(),yticklabels=False,cbar=False,cmap='viridis')","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:33:11.511004Z","iopub.execute_input":"2022-08-15T08:33:11.511721Z","iopub.status.idle":"2022-08-15T08:33:11.729338Z","shell.execute_reply.started":"2022-08-15T08:33:11.511682Z","shell.execute_reply":"2022-08-15T08:33:11.728508Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"Look into the plot, I can say that almost value of Cabin are null. \n\nThe proportion of Age missing is likely small enough for reasonable replacement with some form of imputation. Looking at the Cabin column, it looks like we are just missing too much of that data to do something useful with at a basic level. We'll probably drop this later.","metadata":{}},{"cell_type":"code","source":"# Before processing anything with NULL of age column, I will plot it before coming up with a strategy.\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\ntrain_df['Age'].hist(bins=50, ax=axes[0])\ntrain_df['Age'].to_frame().boxplot(ax=axes[1], vert=False)","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:33:11.730554Z","iopub.execute_input":"2022-08-15T08:33:11.731225Z","iopub.status.idle":"2022-08-15T08:33:12.103193Z","shell.execute_reply.started":"2022-08-15T08:33:11.731192Z","shell.execute_reply":"2022-08-15T08:33:12.101954Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"train_df[[\"Age\"]].info()","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:33:12.106997Z","iopub.execute_input":"2022-08-15T08:33:12.107391Z","iopub.status.idle":"2022-08-15T08:33:12.125199Z","shell.execute_reply.started":"2022-08-15T08:33:12.107358Z","shell.execute_reply":"2022-08-15T08:33:12.123923Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(strategy=\"median\")\nimputer.fit(train_df[[\"Age\"]])\ntrain_df[[\"Age\"]] = imputer.transform(train_df[[\"Age\"]])\ntest_df[[\"Age\"]] = imputer.transform(test_df[[\"Age\"]])","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:33:12.126747Z","iopub.execute_input":"2022-08-15T08:33:12.127942Z","iopub.status.idle":"2022-08-15T08:33:12.146783Z","shell.execute_reply.started":"2022-08-15T08:33:12.127904Z","shell.execute_reply":"2022-08-15T08:33:12.145550Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"train_df[['Age']].info()","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:33:12.148034Z","iopub.execute_input":"2022-08-15T08:33:12.149243Z","iopub.status.idle":"2022-08-15T08:33:12.171684Z","shell.execute_reply.started":"2022-08-15T08:33:12.149204Z","shell.execute_reply":"2022-08-15T08:33:12.170189Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# Standardize the Age column \nimport scipy\nfrom scipy import stats\n\nfor dataset in combine:\n    # transform data\n    dataset['Age'] = stats.zscore(dataset['Age']) + 1\n    \ntrain_df['Age'].head()","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:33:12.173373Z","iopub.execute_input":"2022-08-15T08:33:12.174412Z","iopub.status.idle":"2022-08-15T08:33:12.196556Z","shell.execute_reply.started":"2022-08-15T08:33:12.174364Z","shell.execute_reply":"2022-08-15T08:33:12.194536Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"The test data is not much different from the train data set.","metadata":{}},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:33:12.198912Z","iopub.execute_input":"2022-08-15T08:33:12.199370Z","iopub.status.idle":"2022-08-15T08:33:12.214183Z","shell.execute_reply.started":"2022-08-15T08:33:12.199334Z","shell.execute_reply":"2022-08-15T08:33:12.212939Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:33:12.215673Z","iopub.execute_input":"2022-08-15T08:33:12.216759Z","iopub.status.idle":"2022-08-15T08:33:12.234856Z","shell.execute_reply.started":"2022-08-15T08:33:12.216714Z","shell.execute_reply":"2022-08-15T08:33:12.233691Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"I will use some charts to explore trends and to understand more about the data. Let's see what is the data trying to tell me!","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:33:12.236677Z","iopub.execute_input":"2022-08-15T08:33:12.237411Z","iopub.status.idle":"2022-08-15T08:33:12.250856Z","shell.execute_reply.started":"2022-08-15T08:33:12.237364Z","shell.execute_reply":"2022-08-15T08:33:12.249832Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"sns.set_style('whitegrid')\nsns.countplot(x='Survived',hue='Sex',data=train_data,palette='rainbow')","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:33:12.252063Z","iopub.execute_input":"2022-08-15T08:33:12.253040Z","iopub.status.idle":"2022-08-15T08:33:12.449630Z","shell.execute_reply.started":"2022-08-15T08:33:12.253002Z","shell.execute_reply":"2022-08-15T08:33:12.448310Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"*Women are more likely to survive this disaster.*","metadata":{}},{"cell_type":"code","source":"sns.set_style('whitegrid')\nsns.countplot(x='Survived',hue='Pclass',data=train_data,palette='rainbow')","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:33:12.451249Z","iopub.execute_input":"2022-08-15T08:33:12.451721Z","iopub.status.idle":"2022-08-15T08:33:12.660669Z","shell.execute_reply.started":"2022-08-15T08:33:12.451684Z","shell.execute_reply":"2022-08-15T08:33:12.659439Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"There is not a big difference in the number of survivors in the 3 classes. However, the 3rd class have the huge number of death, it has twice as many deaths as 1st class and 2nd class combined.","metadata":{}},{"cell_type":"code","source":"sns.histplot(data=train_data, x=\"Fare\", hue=\"Survived\",multiple=\"stack\")","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:33:12.662269Z","iopub.execute_input":"2022-08-15T08:33:12.662925Z","iopub.status.idle":"2022-08-15T08:33:13.525260Z","shell.execute_reply.started":"2022-08-15T08:33:12.662859Z","shell.execute_reply":"2022-08-15T08:33:13.523965Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"It seems that the lower the ticket price, the higher the number of deaths. And I also discovered an outlier, this person's fare is twice as much as the person with the second biggest fare.","metadata":{}},{"cell_type":"code","source":"# Number of survivors, deaths in embarked\nsns.countplot(x='Survived',hue='Embarked',data=train_data,palette='rainbow')","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:33:13.526358Z","iopub.execute_input":"2022-08-15T08:33:13.527508Z","iopub.status.idle":"2022-08-15T08:33:13.738264Z","shell.execute_reply.started":"2022-08-15T08:33:13.527469Z","shell.execute_reply":"2022-08-15T08:33:13.737092Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"Most people came from port S, port Q have the least number of people.","metadata":{}},{"cell_type":"code","source":"# Number of Sibsp\nsns.countplot(x='Survived',hue='SibSp',data=train_data,palette='rainbow')","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:33:13.739989Z","iopub.execute_input":"2022-08-15T08:33:13.740685Z","iopub.status.idle":"2022-08-15T08:33:13.997117Z","shell.execute_reply.started":"2022-08-15T08:33:13.740638Z","shell.execute_reply":"2022-08-15T08:33:13.995977Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"**Based on the observations from charts and feature engineering, I can have the following conclusions:**\n* Female have ","metadata":{}},{"cell_type":"markdown","source":"# 2. Building models","metadata":{}},{"cell_type":"code","source":"X_train = train_df.drop(\"Survived\", axis=1)\nY_train = train_df[\"Survived\"]\nX_test  = test_df.drop(\"PassengerId\", axis=1).copy()\nX_train.shape, Y_train.shape, X_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:35:23.582607Z","iopub.execute_input":"2022-08-15T08:35:23.583071Z","iopub.status.idle":"2022-08-15T08:35:23.593785Z","shell.execute_reply.started":"2022-08-15T08:35:23.583034Z","shell.execute_reply":"2022-08-15T08:35:23.592828Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:33:14.010789Z","iopub.execute_input":"2022-08-15T08:33:14.011355Z","iopub.status.idle":"2022-08-15T08:33:14.292089Z","shell.execute_reply.started":"2022-08-15T08:33:14.011320Z","shell.execute_reply":"2022-08-15T08:33:14.290858Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:37:04.120410Z","iopub.execute_input":"2022-08-15T08:37:04.120935Z","iopub.status.idle":"2022-08-15T08:37:04.129891Z","shell.execute_reply.started":"2022-08-15T08:37:04.120897Z","shell.execute_reply":"2022-08-15T08:37:04.128913Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"logreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nacc_log","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:37:06.965675Z","iopub.execute_input":"2022-08-15T08:37:06.966095Z","iopub.status.idle":"2022-08-15T08:37:06.992623Z","shell.execute_reply.started":"2022-08-15T08:37:06.966061Z","shell.execute_reply":"2022-08-15T08:37:06.991194Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"svc = SVC()\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\nacc_svc","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:37:08.701265Z","iopub.execute_input":"2022-08-15T08:37:08.701640Z","iopub.status.idle":"2022-08-15T08:37:08.780322Z","shell.execute_reply.started":"2022-08-15T08:37:08.701609Z","shell.execute_reply":"2022-08-15T08:37:08.778944Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nacc_knn","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:37:48.691735Z","iopub.execute_input":"2022-08-15T08:37:48.692174Z","iopub.status.idle":"2022-08-15T08:37:48.753336Z","shell.execute_reply.started":"2022-08-15T08:37:48.692137Z","shell.execute_reply":"2022-08-15T08:37:48.752091Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"decision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nacc_decision_tree","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:38:11.275204Z","iopub.execute_input":"2022-08-15T08:38:11.275990Z","iopub.status.idle":"2022-08-15T08:38:11.293571Z","shell.execute_reply.started":"2022-08-15T08:38:11.275948Z","shell.execute_reply":"2022-08-15T08:38:11.292718Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"random_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nprint(acc_random_forest)\n\noutput = pd.DataFrame({'PassengerId': test_df.PassengerId, 'Survived': Y_pred})\noutput.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:56:05.151768Z","iopub.execute_input":"2022-08-15T08:56:05.152598Z","iopub.status.idle":"2022-08-15T08:56:05.429787Z","shell.execute_reply.started":"2022-08-15T08:56:05.152540Z","shell.execute_reply":"2022-08-15T08:56:05.428349Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}